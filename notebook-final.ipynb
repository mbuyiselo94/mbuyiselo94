{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Language Identification Hackaton**","metadata":{}},{"cell_type":"markdown","source":"South Africa is a multicultural society that is characterised by its rich linguistic diversity with 11 official langauges. We are building a machine learning algorithm that could determine the natural language that a piece of text is written in (using texts in South Africa languages for the model building).","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing of Packages","metadata":{}},{"cell_type":"code","source":"# Packages for data analysis\nimport pandas as pd\nimport numpy as np\nimport time\n\n# Packages for visualizations\nimport seaborn as sns\nimport matplotlib.style as style\n\n# Packages for preprocessing\nimport nltk\nimport string\nimport re\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Packages for training models\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import metrics\nimport xgboost as xgb\n\n# Model Evaluation Packages\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score\nfrom sklearn.metrics import make_scorer\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Style\nsns.set(font_scale=1.5)\nstyle.use('seaborn-pastel')\nstyle.use('seaborn-poster')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('vader_lexicon')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Loading of Dataset","metadata":{}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing the dataset\ntrain = pd.read_csv('../input/language-identification/train_set.csv')\ntest = pd.read_csv('../input/language-identification/test_set.csv')\nsample_submission = pd.read_csv('../input/language-identification/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train['text'].head(7))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 General Overview of Dataset","metadata":{}},{"cell_type":"code","source":"train.lang_id.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taking general overview at both datasets\nprint('TRAINING DATA')\nprint('============='+('\\n'))\nprint('Shape of the dataset: {}\\n'.format(train.shape))\nprint('Total Number of unique tweets: {}\\n'.format(len(set(train['text']))))\nprint('Total Number of missing values:\\n{}\\n\\n'.format(train.isnull().sum()))\nprint('TEST DATA')\nprint('========='+('\\n'))\nprint('Shape of the dataset: {}\\n'.format(test.shape))\nprint('Total Number of unique tweets: {}\\n'.format(len(set(test['text']))))\nprint('Total Number of missing values:\\n{}\\n' .format(test.isnull().sum()))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    \"\"\"\n    This function uses regular expressions to remove html characters,\n    punctuation, numbers and any extra white space from each text\n    and then converts them to lowercase.\n\n    Input:\n    text: original text\n          datatype: string\n\n    Output:\n    texts: modified text\n           datatype: string\n    \"\"\"\n    # replace the html characters with \" \"\n    text=re.sub('<.*?>', ' ', text)\n#     Removal of numbers\n#    text = re.sub(r'\\d+', ' ', text)\n    # will replace newline with space\n    text = re.sub(\"\\n\",\" \",text)\n    # will convert to lower case\n    text = text.lower()\n    # will split and join the words\n    text=' '.join(text.split())\n    return text\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Application of the function to clean the tweets\ntrain['text'] = train['text'].apply(clean_text)\ntest['text'] = test['text'].apply(clean_text)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace '.txt' with 'text file'\ntrain[\"text\"] = train[\"text\"].str.replace(\".txt\", \" text file\")\ntest[\"text\"] = test[\"text\"].str.replace(\".txt\", \" text file\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Splitting out X (indepedent) and Y (target/dependent) variables\n","metadata":{}},{"cell_type":"code","source":"X = train['text']\ny = train['lang_id']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Splitting of Training and Validation Sets\n","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Model Building","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Setting up Classifiers for Model Training","metadata":{}},{"cell_type":"code","source":"\"\"\"\nNote: Some classifiers were commented out because\nthey run for a very long time, \n\"\"\"\nclassifiers = [LinearSVC(random_state=42),\n               # SVC(),\n               # tree.DecisionTreeClassifier(),\n               # RandomForestClassifier(n_estimators=100, max_depth=2,\n               #                      random_state=0, class_weight=\"balanced\"),\n               # MLPClassifier(alpha=1e-5,\n               #              hidden_layer_sizes=(5, 2),\n               #              random_state=42),\n               LogisticRegression(random_state=42,\n                                  multi_class='ovr',\n                                  n_jobs=1,\n                                  C=1e5,\n                                  max_iter=4000),\n               KNeighborsClassifier(n_neighbors=5),\n               MultinomialNB(),\n               ComplementNB(),\n               SGDClassifier(loss='hinge',\n                             penalty='l2',\n                             alpha=1e-3,\n                             random_state=42,\n                             max_iter=5,\n                             tol=None)\n               # GradientBoostingClassifier()\n               # xgb.XGBClassifier(learning_rate=0.1,\n               #                  n_estimators=1000,\n               #                  max_depth=5,\n               #                  min_child_weight=1,\n               #                  gamma=0,\n               #                  subsample=0.8,\n               #                  colsample_bytree=0.8,\n               #                  nthread=4,\n               #                  seed=27)\n               ]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating Function for Model Building**","metadata":{}},{"cell_type":"code","source":"def models_building(classifiers, X_train, y_train, X_val, y_val):\n    \"\"\"\n    This function takes in a list of classifiers\n    and both the train and validation sets\n    and return a summary of F1-score and\n    processing time as a dataframe\n\n    Input:\n    classifiers: a list of classifiers to train\n                 datatype: list\n    X_train: independent variable for training\n             datatype: series\n    y_train: dependent variable for training\n             datatype: series\n    X_val: independent variable for validation\n           datatype: series\n    y_val: dependent variable for validation\n           datatype: series\n\n    Output:\n    model_summary: F1 Score for all the classifiers\n                   datatype: dataframe\n    \"\"\"\n\n    models_summary = {}\n\n    # Pipeline to balance the classses and then to build the model\n    for clf in classifiers:\n        clf_text = Pipeline([('tfidf', TfidfVectorizer(min_df=1,\n                                                       max_df=0.9,\n                                                       ngram_range=(1, 2))),\n                             ('clf', clf)])\n\n        # Logging the Execution Time for each model\n        start_time = time.time()\n        clf_text.fit(X_train, y_train)\n        predictions = clf_text.predict(X_val)\n        run_time = time.time()-start_time\n\n        # Output for each model\n        models_summary[clf.__class__.__name__] = {\n            'F1-Macro': metrics.f1_score(y_val,\n                                         predictions,\n                                         average='macro'),\n            'F1-Accuracy': metrics.f1_score(y_val, predictions,\n                                            average='micro'),\n            'F1-Weighted': metrics.f1_score(y_val,\n                                            predictions,\n                                            average='weighted'),\n            'Execution Time': run_time}\n\n    return pd.DataFrame.from_dict(models_summary, orient='index')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1.1 Execution of the Classifiers","metadata":{}},{"cell_type":"code","source":"classifiers_df = models_building(classifiers, X_train, y_train, X_val, y_val)\nordered_df = classifiers_df.sort_values('F1-Macro', ascending=False)\nordered_df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1.2 Comparing Classification Methods\n\nThe most performing is the Multinomial Naive Bayes with F1-Macro of 99.9% and accuracy of 99.9% while closely followed by Complement Naive Bayes, Logistic Regression, Linear Support Vector Classifier, Support Vector Machine etc.\n\nWe will proceed with the first two algorithms (to see which will come out better) by applying hyperparameter tunining, as they are the most performing models and considering their execution time.","metadata":{}},{"cell_type":"markdown","source":"## 5.2 Hyperparameter Tuning on Most Performing Models","metadata":{}},{"cell_type":"code","source":"# Refining the train-test split for validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.01)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2.1 Multinomial Naive Bayes","metadata":{}},{"cell_type":"code","source":"# Creating a pipeline for the gridsearch\nparam_grid = {'alpha': [0.1, 1, 5, 10]}  # setting parameter grid\n\ntuned_mnb = Pipeline([('tfidf', TfidfVectorizer(min_df=2,\n                                                max_df=0.9,\n                                                ngram_range=(1, 2))),\n                      ('mnb', GridSearchCV(MultinomialNB(),\n                                           param_grid=param_grid,\n                                           cv=5,\n                                           n_jobs=-1,\n                                           scoring='f1_weighted'))\n                      ])\n\ntuned_mnb.fit(X_train, y_train)  # Fitting the model\n\ny_pred_mnb = tuned_mnb.predict(X_val)  # predicting the fit on validation set\n\nprint(classification_report(y_val, y_pred_mnb))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Creating File for Submission","metadata":{}},{"cell_type":"code","source":"submission_df = pd.DataFrame(test['index'])\nsubmission_df['lang_id'] = tuned_mnb.predict(test['text'])\nsubmission_df.to_csv('submission_tuned_multinomial_NB.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Conclusion","metadata":{}},{"cell_type":"markdown","source":"Several algorithms were tried and Multinomial Naive Bayes classifier was the most performing. It performed very well on the training and validation datasets with an accuracy score of over 99% and F1 Macro score of over 99%. After testing the fitted model on the held-out/unseen dataset, it was able to predict the classes of languages with an F1 Score of about 97%.","metadata":{}},{"cell_type":"markdown","source":"# 7. References","metadata":{}},{"cell_type":"markdown","source":"1. Hyperparameters and Model Validation (An overview of classification model hyperparameters, hyperparameter tuning, and model validation) - Explore Data Science Academy\n2. https://scikit-learn.org/stable/modules/grid_search.html","metadata":{}}]}